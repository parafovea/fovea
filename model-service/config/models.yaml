# Model Configuration
# Defines which models to use for each task type and their loading parameters

models:
  video_summarization:
    selected: "qwen-2-5-vl-7b"
    options:
      qwen-2-5-vl-7b:
        model_id: "Qwen/Qwen2.5-VL-7B-Instruct"
        quantization: "4bit"
        framework: "sglang"
        vram_gb: 8
        speed: "fast"
        description: "Compact VLM, ungated, fits well on A10G with room for inference"
      llama-4-maverick:
        model_id: "meta-llama/Llama-4-Maverick"
        quantization: "4bit"
        framework: "sglang"
        vram_gb: 62
        speed: "fast"
        description: "MoE model with 17B active parameters, multimodal, 10M context"
      gemma-3-27b:
        model_id: "google/gemma-3-27b-it"
        quantization: "4bit"
        framework: "sglang"
        vram_gb: 14
        speed: "very_fast"
        description: "Document analysis, OCR, multilingual support (gated)"
      internvl3-78b:
        model_id: "OpenGVLab/InternVL3-78B"
        quantization: "4bit"
        framework: "sglang"
        vram_gb: 40
        speed: "medium"
        description: "SOTA benchmarks, scientific reasoning"
      pixtral-large:
        model_id: "mistralai/Pixtral-Large-Instruct-2411"
        quantization: "4bit"
        framework: "sglang"
        vram_gb: 62
        speed: "medium"
        description: "Long context (128k), batch processing"
      qwen2-5-vl-72b:
        model_id: "Qwen/Qwen2.5-VL-72B-Instruct"
        quantization: "4bit"
        framework: "sglang"
        vram_gb: 36
        speed: "fast"
        description: "Proven stable, good baseline"
      claude-sonnet-4-5:
        model_id: "claude-sonnet-4-5"
        framework: "external_api"
        provider: "anthropic"
        api_endpoint: "https://api.anthropic.com/v1/messages"
        requires_api_key: true
        speed: "fast"
        description: "Anthropic Claude Sonnet 4.5, vision capable, API-based"
      gpt-4o:
        model_id: "gpt-4o"
        framework: "external_api"
        provider: "openai"
        api_endpoint: "https://api.openai.com/v1/chat/completions"
        requires_api_key: true
        speed: "fast"
        description: "OpenAI GPT-4 Omni, vision capable, API-based"
      gemini-2-5-flash:
        model_id: "gemini-2.5-flash"
        framework: "external_api"
        provider: "google"
        api_endpoint: "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent"
        requires_api_key: true
        speed: "very_fast"
        description: "Google Gemini 2.5 Flash, multimodal, API-based"

  ontology_augmentation:
    selected: "qwen-2-5-7b"
    options:
      qwen-2-5-7b:
        model_id: "Qwen/Qwen2.5-7B-Instruct"
        quantization: "4bit"
        framework: "sglang"
        vram_gb: 4
        speed: "very_fast"
        description: "Compact text LLM, ungated, efficient for ontology tasks"
      qwen-2-5-32b:
        model_id: "Qwen/Qwen2.5-32B-Instruct"
        quantization: "4bit"
        framework: "sglang"
        vram_gb: 16
        speed: "fast"
        description: "Qwen 2.5 32B, ungated, excellent reasoning"
      llama-4-scout:
        model_id: "meta-llama/Llama-4-Scout"
        quantization: "4bit"
        framework: "sglang"
        vram_gb: 55
        speed: "very_fast"
        description: "MoE model with 17B active, 10M context, multimodal"
      llama-3-3-70b:
        model_id: "meta-llama/Llama-3.3-70B-Instruct"
        quantization: "4bit"
        framework: "sglang"
        vram_gb: 35
        speed: "fast"
        description: "Matches 405B quality, proven performance"
      deepseek-v3:
        model_id: "deepseek-ai/DeepSeek-V3"
        quantization: "4bit"
        framework: "sglang"
        vram_gb: 85
        speed: "fast"
        description: "MoE with 37B active, best reasoning, scientific tasks"
      gemma-3-27b-text:
        model_id: "google/gemma-3-27b-it"
        quantization: "4bit"
        framework: "sglang"
        vram_gb: 14
        speed: "very_fast"
        description: "Lightweight, fast iteration"
      claude-sonnet-4-5:
        model_id: "claude-sonnet-4-5"
        framework: "external_api"
        provider: "anthropic"
        api_endpoint: "https://api.anthropic.com/v1/messages"
        requires_api_key: true
        speed: "fast"
        description: "Anthropic Claude Sonnet 4.5, text-only, API-based"
      gpt-4o:
        model_id: "gpt-4o"
        framework: "external_api"
        provider: "openai"
        api_endpoint: "https://api.openai.com/v1/chat/completions"
        requires_api_key: true
        speed: "fast"
        description: "OpenAI GPT-4 Omni, text-only, API-based"
      gemini-2-5-flash:
        model_id: "gemini-2.5-flash"
        framework: "external_api"
        provider: "google"
        api_endpoint: "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent"
        requires_api_key: true
        speed: "very_fast"
        description: "Google Gemini 2.5 Flash, text-only, API-based"

  object_detection:
    selected: "yolo-world-v2"
    options:
      yolo-world-v2:
        model_id: "yolov8l-worldv2.pt"
        framework: "pytorch"
        vram_gb: 2
        speed: "real_time"
        fps: 52
        description: "Speed and accuracy balance, image prompts"
      grounding-dino-1-5:
        model_id: "IDEA-Research/grounding-dino-1.5-pro"
        framework: "pytorch"
        vram_gb: 4
        speed: "medium"
        fps: 20
        description: "Open-world, zero-shot, 52.5 AP"
      owlv2:
        model_id: "google/owlv2-large-patch14-ensemble"
        framework: "pytorch"
        vram_gb: 6
        speed: "medium"
        fps: 15
        description: "Scaled training, rare classes"
      florence-2:
        model_id: "microsoft/Florence-2-large"
        framework: "pytorch"
        vram_gb: 2
        speed: "fast"
        fps: 30
        description: "Unified vision, captioning"

  video_tracking:
    selected: "samurai"
    options:
      samurai:
        model_id: "yangchris11/samurai"
        framework: "pytorch"
        vram_gb: 3
        speed: "real_time"
        description: "Motion-aware, occlusion handling, 7.1% better"
      sam2long:
        model_id: "Mark12Ding/SAM2Long"
        framework: "pytorch"
        vram_gb: 3
        speed: "real_time"
        description: "Long videos, error accumulation fix, 5.3% better"
      sam2-1:
        model_id: "facebook/sam2.1-hiera-large"
        framework: "pytorch"
        vram_gb: 3
        speed: "real_time"
        description: "Baseline, proven stable"
      yolo11n-seg:
        model_id: "ultralytics/yolo11n-seg"
        framework: "pytorch"
        vram_gb: 1
        speed: "very_fast"
        description: "Lightweight, speed-critical tasks"

  audio_transcription:
    selected: "whisper-v3-turbo"
    options:
      whisper-v3-turbo:
        model_id: "openai/whisper-large-v3-turbo"
        framework: "whisper"
        vram_gb: 5
        speed: "very_fast"
        description: "OpenAI Whisper v3 Turbo, 8x faster, multilingual"
      faster-whisper-large-v3:
        model_id: "Systran/faster-whisper-large-v3"
        framework: "faster_whisper"
        vram_gb: 3
        speed: "fast"
        description: "CTranslate2 optimized, 4x faster than default Whisper"
      whisper-large-v3:
        model_id: "openai/whisper-large-v3"
        framework: "whisper"
        vram_gb: 10
        speed: "medium"
        description: "Full Whisper large v3, highest accuracy"
      assemblyai-universal:
        model_id: "best"
        framework: "external_api"
        provider: "assemblyai"
        api_endpoint: "https://api.assemblyai.com/v2"
        requires_api_key: true
        speed: "fast"
        description: "AssemblyAI Universal-2, speaker diarization, sentiment"
      deepgram-nova-3:
        model_id: "nova-3"
        framework: "external_api"
        provider: "deepgram"
        api_endpoint: "https://api.deepgram.com/v1"
        requires_api_key: true
        speed: "very_fast"
        description: "Deepgram Nova-3, streaming capable, highest accuracy"
      revai:
        model_id: "default"
        framework: "external_api"
        provider: "revai"
        api_endpoint: "https://api.rev.ai/speechtotext/v1"
        requires_api_key: true
        speed: "fast"
        description: "Rev AI Speech-to-Text, human-level accuracy"
      gladia:
        model_id: "default"
        framework: "external_api"
        provider: "gladia"
        api_endpoint: "https://api.gladia.io/v2"
        requires_api_key: true
        speed: "fast"
        description: "Gladia Audio API, multilingual, code-switching"
      aws-transcribe:
        model_id: "default"
        framework: "external_api"
        provider: "aws"
        api_endpoint: "https://transcribe.us-east-1.amazonaws.com"
        requires_api_key: true
        speed: "medium"
        description: "AWS Transcribe, speaker diarization, medical/legal vocab"
      google-speech:
        model_id: "chirp_2"
        framework: "external_api"
        provider: "google"
        api_endpoint: "https://speech.googleapis.com/v2"
        requires_api_key: true
        speed: "fast"
        description: "Google Speech-to-Text v2 with Chirp 2, multilingual"
      azure-speech:
        model_id: "default"
        framework: "external_api"
        provider: "azure"
        api_endpoint: "https://YOUR_REGION.api.cognitive.microsoft.com"
        requires_api_key: true
        speed: "fast"
        description: "Azure Speech Services, real-time streaming"

  claim_extraction:
    selected: "qwen-2-5-7b"
    options:
      qwen-2-5-7b:
        model_id: "Qwen/Qwen2.5-7B-Instruct"
        quantization: "4bit"
        framework: "transformers"
        vram_gb: 4
        speed: "very_fast"
        description: "Compact text LLM, ungated, efficient for claim extraction"
      llama-4-scout:
        model_id: "meta-llama/Llama-4-Scout"
        quantization: "4bit"
        framework: "transformers"
        vram_gb: 55
        speed: "very_fast"
        description: "MoE model with 17B active, 10M context, excellent for claim decomposition"
      deepseek-v3:
        model_id: "deepseek-ai/DeepSeek-V3"
        quantization: "4bit"
        framework: "transformers"
        vram_gb: 85
        speed: "fast"
        description: "MoE with 37B active, best reasoning for complex claims"
      gemma-3-27b:
        model_id: "google/gemma-3-27b-it"
        quantization: "4bit"
        framework: "transformers"
        vram_gb: 14
        speed: "very_fast"
        description: "Lightweight, fast claim extraction (gated)"

  claim_synthesis:
    selected: "qwen-2-5-7b"
    options:
      qwen-2-5-7b:
        model_id: "Qwen/Qwen2.5-7B-Instruct"
        quantization: "4bit"
        framework: "transformers"
        vram_gb: 4
        speed: "very_fast"
        description: "Compact text LLM, ungated, efficient for claim synthesis"
      llama-4-scout:
        model_id: "meta-llama/Llama-4-Scout"
        quantization: "4bit"
        framework: "transformers"
        vram_gb: 55
        speed: "very_fast"
        description: "MoE model with 17B active, 10M context, excellent for narrative synthesis"
      deepseek-v3:
        model_id: "deepseek-ai/DeepSeek-V3"
        quantization: "4bit"
        framework: "transformers"
        vram_gb: 85
        speed: "fast"
        description: "MoE with 37B active, best reasoning for complex multi-source synthesis"
      gemma-3-27b:
        model_id: "google/gemma-3-27b-it"
        quantization: "4bit"
        framework: "transformers"
        vram_gb: 14
        speed: "very_fast"
        description: "Lightweight, fast synthesis (gated)"

  speaker_diarization:
    selected: "pyannote-3-1"
    options:
      pyannote-3-1:
        model_id: "pyannote/speaker-diarization-3.1"
        framework: "pyannote"
        vram_gb: 2
        speed: "medium"
        description: "Pyannote Audio 3.1, speaker segmentation and clustering"

  voice_activity_detection:
    selected: "silero-vad"
    options:
      silero-vad:
        model_id: "silero_vad"
        framework: "pytorch"
        vram_gb: 0.1
        speed: "real_time"
        description: "Silero VAD v5, lightweight speech detection"

inference:
  max_memory_per_model: "auto"
  offload_threshold: 0.85
  warmup_on_startup: true
  default_batch_size: 1
  max_batch_size: 8
